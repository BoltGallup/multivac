install.packages("cdcfluview")
library(cdcfluview)
state_data = get_state_data(years = as.numeric(format(Sys.Date(2014), "%Y")))
state_data = get_state_data(2014)
View(state_data)
scrape_indeed_reviews <- function(url_path, page) {
# construct the url based on two parameters
# the base url path and the pagination
# url_path should be the base url for the Indeed review page
# page should be the last review before the results we want to pull
# e.g. for the third page of resuls use page = 40
url <- paste0(url_path, "?start=", page,"&sort=date_desc")
# read and download the page's underlying html
payload <- read_html(url)
date <- payload %>%
html_nodes(".cmp-review-date-created") %>%
html_text() %>%
as.data.frame() %>%
rename(date = ".")
date <- date %>%
mutate(date = as.character(date),
date = lubridate::mdy(date))
# scrape location
location <- payload %>%
html_nodes(".cmp-reviewer-job-location") %>%
html_text() %>%
as.data.frame() %>%
rename(location = ".")
# scrape job role and employee status
job_status <- payload %>%
html_nodes(".cmp-reviewer-job-title") %>%
html_text() %>%
as.data.frame() %>%
rename(job = ".") %>%
mutate(job = as.character(job),
job_status = ifelse(grepl("current employee",
x = job, ignore.case = TRUE),
"Current Employee",
ifelse(grepl("former employee",
x = job, ignore.case = TRUE),
"Former Employee", NA)))
job <- as.data.frame(str_split(job_status$job, pattern = "\\(",
simplify = TRUE))
job_status$job <- job$V1
# scrape the content of the review
reviews <- payload %>%
html_nodes(".cmp-review-description") %>%
html_text() %>%
as.data.frame() %>%
rename(review = ".") %>%
mutate(review = as.character(review),
review = gsub("\\s\\smore...", "", review),
review = gsub("\\s\\sless", "", review))
# scrape the underlying rating given by the reviewer
ratings <- payload %>%
html_nodes(".cmp-ratingNumber") %>%
html_text() %>%
as.data.frame() %>%
rename(rating = ".") %>%
mutate(rating = as.numeric(as.character(rating)))
# combine each the elements scraped into a single data frame
combined <- data_frame(date = date$date,
location = location$location,
job = job_status$job,
employee_status = job_status$job_status,
rating = ratings$rating,
review = reviews$review)
return(combined)
# pause for a random amount of time -- between .99 and .01 seconds
# useful when used in a loop for multiple pages and/or multiple locations
Sys.sleep(sample(seq(from = 5, to = 10, by = 1), 1))
}
install.packages('rvest')
install.packages('magrittr')
install.packages('tidyverse')
install.packages('stringr')
install.packages('httr')
install.packages('lubridate')
require(rvest)
require(magrittr)
require(tidyverse)
require(stringr)
require(httr)
require(lubridate)
scrape_indeed_reviews("https://www.indeed.com/cmp/Google/reviews", page = 0) %>% View()
scrape_indeed_reviews("https://www.indeed.com/cmp/United-States-Postal-Service/reviews", page = 0) %>% View()
indeed_reviews_for_business <- function(url_path, pages = "all") {
if (pages == "all") {
# scrape company review page to determine the number of pages
url <- paste0(url_path, "?start=0&sort=date_desc")
payload <- xml2::read_html(url)
max_page_nummber <- payload %>%
html_nodes(".cmp-Pagination-link") %>%
html_text() %>%
str_subset(pattern = "[0-9]") %>%
as.numeric() %>%
max()
pagination <- data_frame(pages = seq(0,(max_page_nummber*20), by = 20))
} else {
pagination <- data_frame(pages = pages)
}
results_indeed_reviews <- list()
for (i in seq_along(pagination$pages)) {
tryCatch(
{
payload <- scrape_indeed_reviews(url_path = url_path,
page = pagination$pages[i])
payload$page <- pagination$pages[i]
results_indeed_reviews[[i]] <- payload
},
error=function(cond) {
message(paste0("not completed: ",
paste0(url_path, "?start=",
pagination$pages[i],
"&sort=date_desc")))
return(NA)
}
)
}
results <- bind_rows(results_indeed_reviews)
results <- results %>%
distinct(date, location, job, employee_status,
rating, review, .keep_all = TRUE)
url_page_errors <- setdiff(pagination$pages, unique(results$page))
if (length(url_page_errors) != 0) {
url_errors <- lapply(url_page_errors, function(x) paste0(url_path, "?start=",x,"&sort=date_desc"))
url_errors <- as.data.frame(do.call(rbind, url_errors))
names(url_errors) <- "urls_not_collected"
url_errors$pages <- url_page_errors
payload_list <- list(results = results,
url_errors = url_errors)
return(payload_list)
} else {
return(results)}
}
test <- indeed_reviews_for_business(url_path = "https://www.indeed.com/cmp/United-States-Postal-Services/reviews", pages = seq(0,200,by = 20))
View(test)
test <- indeed_reviews_for_business(url_path = "https://www.indeed.com/cmp/United-States-Postal-Services/reviews", pages = seq(0,19000,by = 20))
url_path = 'https://www.indeed.com/cmp/United-States-Postal-Services/reviews'
url <- paste0(url_path, "?start=0&sort=date_desc")
payload <- xml2::read_html(url)
View(payload)
dim(test)
ncol(test)
nrow(test)
max_page_nummber <- payload %>%
html_nodes(".cmp-Pagination-link") %>%
html_text()
max_page_nummber <- payload %>%
html_nodes(".cmp-Pagination-link") %>%
html_text() %>%
str_subset(pattern = "[0-9]")
max_page_nummber <- payload %>%
html_nodes(".cmp-Pagination-link") %>%
html_text() %>%
str_subset(pattern = "[0-9]") %>%
as.numeric() %>%
max()
pagination <- data_frame(pages = seq(0,(max_page_nummber*20), by = 20))
View(pagination)
50*10
100*10
500*10
setwd("~/Desktop/PEW/Template_Processing")
library(foreign)
? read.spss
data = read.spss('PEW 2019 EUROPEAN CORE.sav')
data = read.spss('PEW 2019 EUROPEAN CORE.sav',to.data.frame = TRUE)
data.labels
data.variable.labels
var_labels <- attr(data, "variable.labels")
var_labels[0]
var_labels[1]
var_labels[2]
var_labels[3]
setwd("~/Desktop/ASKE_MULTIVAC/multivac/notebooks/GloVE_Embed")
rm
clear
clc
rm(list=ls())
source('glove_analysis.R')
