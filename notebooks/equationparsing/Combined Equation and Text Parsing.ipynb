{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "## Load packages\n",
    "####################################################################\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re as reg\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sympy.parsing.sympy_parser import (parse_expr, standard_transformations, implicit_multiplication_application)\n",
    "from sympy.parsing.latex import parse_latex  \n",
    "from sympy import *\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "####################################################################\n",
    "## Load NLP\n",
    "####################################################################\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('en')\n",
    "except:\n",
    "    nlp = spacy.load('E:/Users/nasser_qadri/AppData/Local/conda/conda/envs/multivac/Lib/site-packages/en_core_web_sm/en_core_web_sm-2.0.0')\n",
    "nlp.max_length=1000000000\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "## Global variables, mostly regex for LateX parsing and text cleaning\n",
    "####################################################################\n",
    "# Use this to store LateX code \n",
    "latexMap = {}\n",
    "\n",
    "## Use this for debugging to keep track of the different governors\n",
    "listGovs = []\n",
    "\n",
    "#LateX identifiers\n",
    "latexBlock= reg.compile('\\$\\$.*?\\$\\$')\n",
    "latexInline= reg.compile('\\\\\\\\.*?\\\\\\\\\\)')\n",
    "\n",
    "\n",
    "### Regex for cleaning Python\n",
    "re_citationsNumeric = reg.compile('(\\[\\d+)(,\\s*\\d+)*]')\n",
    "re_url= reg.compile(r'((http|ftp|https):\\/\\/)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n",
    "author = r\"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "etal = r\"(?:et al.?)\"\n",
    "additional = r\"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
    "year_num = r\"(?:19|20)[0-9][0-9]\"\n",
    "page_num = r\"(?:, p.? [0-9]+)?\"  # Always optional\n",
    "year = \"(?:, *\"+year_num+page_num+\"| *\\(\"+year_num+page_num+\"\\))\"\n",
    "re_intextcite = reg.compile(r\"(\" + author + additional+\"*\" + year + \")\")\n",
    "#re_intextcite = reg.compile(r\"((?:[A-Z][A-Za-z'`-é-]+)(?:,? (?:(?:and |& )?(?:[A-Z][A-Za-z'`-]+)|(?:et al.?)))*(?:,* *(?:19|20)[0-9][0-9](?:, p.? [0-9]+)?| *\\\\((?:19|20)[0-9][0-9](?:, p.? [0-9]+)?\\\\)))\")\n",
    "#re_intextcite = reg.compile(r\"((?:[A-Za-z][A-Za-z'`-é-]+)(?:,? (?:(?:and |& )?(?:[A-Za-z][A-Za-z'`-é-]+)|(?:et al.?)))*(?:,* *((?:19|20)[0-9][0-9][a-z]*)(, (\\d+))*(?:, p.? [0-9]+)?| *\\\\((?:19|20)[0-9][0-9][a-z](?:, p.? [0-9]+)?\\\\)))\")\n",
    "re_intextcite = reg.compile(r\"((?:[A-Za-z][A-Za-z'`-éü-]+)(?:,? (?:(?:and |& )?(?:[A-Za-z][A-Za-z'`-éü-]+)|(?:et al.?)))*(?:,* *((?:19|20)[0-9][0-9][a-z]*)(\\s*&\\s*[0-9]*[a-z]*)*(, (\\d+))*(?:, p.? [0-9]+)?| *\\\\((?:19|20)[0-9][0-9][a-z](\\s*&)(?:, p.? [0-9]+)?\\\\)))\")\n",
    "\n",
    "#re_emptyCite = reg.compile(r\"\\((\\s*;*\\s*)+\\)\")\n",
    "re_emptyCite = reg.compile(r\"\\(([\\s]*[;]+[\\s]*)+\\)\")\n",
    "re_emptyEg = reg.compile(r'\\(e.g.[\\s*;\\s*]*[,]*\\s*\\)')\n",
    "re_clickHere = reg.compile(r'Click here[^.]*\\.')\n",
    "re_cid=reg.compile(r\"\\(cid:\\d+\\)\")\n",
    "re_email = reg.compile(r\"[\\w.-]+@[\\w.-]+\")\n",
    "re_emptyParens = reg.compile(r\"\\(\\s*\\)\")\n",
    "re_emptySee = reg.compile(r\"\\(see(\\s)*\\)\")\n",
    "re_sponsors = reg.compile(r'(This work was supported).+')\n",
    "re_arxivHeader = reg.compile(r\"(a r X i v).*?(?=[a-zA-Z]{2,})\")\n",
    "re_vixraHeader = reg.compile(r\"^(\\s?.?\\s)+(v i X r a)\")\n",
    "re_hyphenatedWords = reg.compile(r'\\S(?=\\S*[-]\\s)([a-zA-Z-]+)(\\s)[A-za-z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityMatcher(object):\n",
    "    '''\n",
    "    This creates a step in Spacy's NLP pipeine to recognize and handle equations\n",
    "    '''\n",
    "    name = 'entity_matcher'\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        return doc\n",
    "    \n",
    "\n",
    "def load_data(picklePath = None):\n",
    "    \"\"\"Load data - if picklePath is specified, load the pickle. Else, try json file.\n",
    "    This returns the JSON file as well as a list of document texts \n",
    "    \"\"\"\n",
    "    if picklePath is not None:\n",
    "        l_docs = pickle.load(open(picklePath, \"rb\" ))\n",
    "    else:\n",
    "\n",
    "        ## Read JSON data into the datastore variable - this comes from Peter and Domonique's effort. Don\n",
    "        with open('../../data/20181212.json', 'r') as f:\n",
    "            datastore = json.load(f)\n",
    "            \n",
    "        \n",
    "        ## These were some bad files - nothing substantive in them, or they were retrieved in bad format\n",
    "        for e in ['1805.10677v1', '0911.5378v1']: \n",
    "            datastore.pop(e)\n",
    "\n",
    "        ## Extract texts\n",
    "        l_docs = [value['text'] for key,value in list(datastore.items())[0:] if value['text'] ]\n",
    "        \n",
    "    print('# of documents: ', len(l_docs))\n",
    "    \n",
    "    return datastore, l_docs\n",
    "    \n",
    "    \n",
    "def retrieve_JSON_output(l_docs):\n",
    "    \"\"\"Create a JSON output of dependency trees - this has been replaced with a text output (instead of a JSON output). \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    dependencyDocuments = []\n",
    "    \n",
    "    for di, doc in enumerate(l_docs[0:]):    \n",
    "        for sent in list(doc.sents)[0:]:\n",
    "            sentenceObj = {}\n",
    "            sentenceObj['sentence']=sent.text\n",
    "            words = []\n",
    "\n",
    "            for token in sent:        \n",
    "                wordObj = {\n",
    "                    'tokenText':token.text,\n",
    "                     'tokenTag':token.tag_,\n",
    "                     'tokenDep':token.dep_,\n",
    "                     'tokenHeadText':token.head.text,\n",
    "                     'tokenHeadTag':token.head.tag_\n",
    "                }\n",
    "                words.append(wordObj)\n",
    "                #print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "\n",
    "            sentenceObj['words'] = words\n",
    "            sentences.append(sentenceObj)\n",
    "\n",
    "        docObject = {}\n",
    "        docObject['id']=di\n",
    "        docObject['sentences']=sentences\n",
    "        dependencyDocuments.append(docObject)\n",
    "    \n",
    "    return dependencyDocuments\n",
    "\n",
    "\n",
    "\n",
    "def getAdjustmentPosition(tokenPosition, adjustmentDictionary):\n",
    "    '''This determines the adjustment position for DEP files, because things get reordered when there are equations\n",
    "    '''\n",
    "    if len(adjustmentDictionary)>1:\n",
    "        for key, val in sorted(list(adjustmentDictionary.items()), key=lambda x:x, reverse=True):\n",
    "            if tokenPosition>key:\n",
    "                return val\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def create_parse_files(doc, docNum, writeFile = True, pathToFolders=''):\n",
    "    \"\"\" Creates parse files and stores them in the folder passed when writeFile=True and pathToFolders is provided\n",
    "        The following file types are created\n",
    "            * dep -- for dependencies\n",
    "            * input -- for POS tagging\n",
    "            * morph -- lemmatized words\n",
    "    \"\"\"\n",
    "    \n",
    "    d_documentData = {\n",
    "        'depData' : [],\n",
    "        'posData' : [],\n",
    "        'morData' : []\n",
    "    }\n",
    "    \n",
    "    l_depSentences = [] # for dependencies\n",
    "    l_posSentences = [] # for POS tagging\n",
    "    l_morSentences = [] # for morphology/lemmatization \n",
    "\n",
    "    for sent in list(doc.sents)[0:]:\n",
    "\n",
    "        l_depTokens_tuples=[]\n",
    "        l_depTokens=[]\n",
    "        l_posTokens=[]\n",
    "        l_morTokens=[]\n",
    "        l_depTokens_latex_tuples=[]\n",
    "        l_depTokens_latex=[]\n",
    "        l_posTokens_latex=[]\n",
    "        l_morTokens_latex=[]\n",
    "\n",
    "        adjustedPosition = 0\n",
    "        adjustmentDictionary = {0:0}\n",
    "        for token in sent:\n",
    "            \n",
    "            if 'LateXEquation' in token.head.text:\n",
    "                pass\n",
    "            \n",
    "            if  (token.text==' '):\n",
    "                adjustedPosition= adjustedPosition-1\n",
    "                adjustmentDictionary[(token.i - sent.start +1)]=adjustedPosition\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            elif 'LateXEquation' in token.text:\n",
    "                \n",
    "                l_depTokens_latex_sub_tuples, l_posTokens_latex_sub, l_morTokens_latex_sub = latexParsing(\n",
    "                    token, token.i - sent.start  + 1 + adjustedPosition)\n",
    "                \n",
    "                # Need to adjust position so that it we add all the new tokens, then subtract 1 for LateXEquation##\n",
    "                \n",
    "                adjustedPosition = adjustedPosition + (len(l_posTokens_latex_sub) -1)\n",
    "                adjustmentDictionary[(token.i - sent.start +1)]=adjustedPosition\n",
    "                \n",
    "\n",
    "                ## Go backwards and make sure all the previous ones are good\n",
    "                new_l_depTokens_tuples = []\n",
    "                \n",
    "                \n",
    "                for depSet in l_depTokens_tuples:\n",
    "                    t1, t2, t3 = depSet\n",
    "                    t2=list(t2)\n",
    "                    t3=list(t3)\n",
    "                    \n",
    "                    #current position is the threshold for change\n",
    "                    if t2[1]> (token.i - sent.start  + 1):\n",
    "                        t2[1] = t2[1]+adjustedPosition\n",
    "                    if t3[1]> (token.i - sent.start  + 1):\n",
    "                        t3[1] = t3[1]+adjustedPosition\n",
    "                                \n",
    "                    adjustedTuple = (t1,tuple(t2),tuple(t3))\n",
    "                    new_l_depTokens_tuples.append(adjustedTuple)\n",
    "                    \n",
    "                l_depTokens_tuples = new_l_depTokens_tuples\n",
    "                \n",
    "                # Now add to the master list\n",
    "                l_depTokens_tuples = l_depTokens_tuples + l_depTokens_latex_sub_tuples\n",
    "                l_posTokens = l_posTokens + l_posTokens_latex_sub\n",
    "                l_morTokens = l_morTokens + l_morTokens_latex_sub\n",
    "                \n",
    "                    \n",
    "            else:\n",
    "                ## For rest of sentence \n",
    "                ## For dependency trees\n",
    "                childTokenPosition = token.i - sent.start  + 1\n",
    "                headTokenPosition =  token.head.i - sent.start +1 \n",
    "\n",
    "                if token.dep_ not in ['ROOT','punct']:\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    l_depTokens_tuples.append( ( token.dep_ , (token.head.text, headTokenPosition + \n",
    "                                                               getAdjustmentPosition(headTokenPosition, adjustmentDictionary)), \n",
    "                                         (token.text, childTokenPosition + \n",
    "                                          getAdjustmentPosition(childTokenPosition, adjustmentDictionary)) ) )\n",
    "                    \n",
    "                    \n",
    "                ## For POS\n",
    "                l_posTokens.append(\"{0}_{1}\".format(token, token.tag_))  \n",
    "                #print(token.tag_)\n",
    "\n",
    "                ## For Morphologies\n",
    "                l_morTokens.append(token.lemma_)\n",
    "        \n",
    "        \n",
    "        ## Need to Parse out DEPTokens from tuples out to text\n",
    "        for depSet in l_depTokens_tuples:\n",
    "            t1, t2, t3 = depSet\n",
    "            l_depTokens.append(\"{0}({1}-{2}, {3}-{4})\".format(t1, t2[0],t2[1],t3[0], t3[1]))\n",
    " \n",
    "        for depSet in l_depTokens_latex_tuples:\n",
    "            t1, t2, t3 = depSet\n",
    "            l_depTokens_latex.append(\"{0}({1}-{2}, {3}-{4})\".format(t1, t2[0],t2[1],t3[0], t3[1]))\n",
    " \n",
    "        \n",
    "        \n",
    "        l_depSentences.append(\"\\n\".join(l_depTokens + l_depTokens_latex))\n",
    "        l_posSentences.append(\"\\n\".join(l_posTokens))\n",
    "        l_morSentences.append(\"\\n\".join(l_morTokens))\n",
    "        \n",
    "\n",
    "    d_documentData['depData'].append(l_depSentences)\n",
    "    d_documentData['posData'].append(l_posSentences)\n",
    "    d_documentData['morData'].append(l_morSentences)\n",
    "\n",
    "    if writeFile:\n",
    "        with open(pathToFolders+'\\\\dep\\\\{0:04d}.dep'.format(docNum), \"w\", encoding='utf8') as text_file:\n",
    "            text_file.write('\\n\\n'.join(l_depSentences))\n",
    "        with open(pathToFolders+'\\\\input\\\\{0:04d}.input'.format(docNum), \"w\", encoding='utf8') as text_file:\n",
    "            text_file.write('\\n\\n'.join(l_posSentences))\n",
    "        with open(pathToFolders+'\\\\morph\\\\{0:04d}.morph'.format(docNum), \"w\", encoding='utf8') as text_file:\n",
    "            text_file.write('\\n\\n'.join(l_morSentences))\n",
    "\n",
    "        print('Files written to folder:', pathToFolders)\n",
    "    return d_documentData\n",
    "\n",
    "\n",
    "def replace_latex(m):\n",
    "    '''\n",
    "    Replace LateX equations with placeholder token, with format LateXEquation\n",
    "    '''\n",
    "    latexStr = m.group()\n",
    "    latexStr = cleaned_latex(latexStr)\n",
    "    \n",
    "    aggregatedMapKey = ''\n",
    "    if ('cid:' not in latexStr):\n",
    "        #print('\\n',latexStr)\n",
    "        \n",
    "        #Sometimes there are multiple equations within each block. Get those here\n",
    "        latexArray = latexStr.split(', \\\\\\\\')\n",
    "        for latexItem in latexArray: \n",
    "            counter = len(latexMap)\n",
    "            thisMapKey = ' LateXEquation'+str(counter) + ' '\n",
    "            aggregatedMapKey = aggregatedMapKey +  thisMapKey\n",
    "            latexMap[thisMapKey.replace(' ','')] = latexItem ## in this case, 'key' is the latex code\n",
    "\n",
    "        return (aggregatedMapKey)\n",
    "        \n",
    "\n",
    "def extract_and_replace_latex(doc, docNum):\n",
    "    '''\n",
    "    Find and extract LateX, start with blockquote and then do inline\n",
    "    '''\n",
    "    \n",
    "    doc = reg.sub(latexBlock, replace_latex, doc)\n",
    "    doc = reg.sub(latexInline, replace_latex, doc)\n",
    "\n",
    "\n",
    "    return doc\n",
    "\n",
    "\n",
    "def cleaned_latex(s):    \n",
    "    '''LateX requires some cleaning from original file format\n",
    "    '''\n",
    "    s=s.replace('$$','')\n",
    "    s = reg.sub(r'\\\\begin{array}{.*?}', '', s)\n",
    "    s = reg.sub(r'\\\\end{array}', '', s)\n",
    "    s = reg.sub(r'\\\\begin{aligned}', '', s)\n",
    "    s = reg.sub(r'\\\\end{aligned}', '', s)\n",
    "    s=s.replace('&=&','=')\n",
    "    s=s.replace('\\(','(')\n",
    "    s=s.replace('\\)',')')\n",
    "    s=s.lstrip().rstrip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_doc(doc):   \n",
    "    '''\n",
    "    Clean individual documents and remove citations, URLs, emails, other trivial content. Returns cleaned doc\n",
    "    '''\n",
    "    doc = reg.sub(re_cid, ' ', doc)\n",
    "    doc = reg.sub(re_citationsNumeric, ' NumericCitation ', doc)\n",
    "    doc = reg.sub(re_url, ' ', doc)\n",
    "    doc = reg.sub(re_intextcite, ' Citation ', doc)\n",
    "    doc = reg.sub(re_emptyCite, ' ', doc)\n",
    "    doc = reg.sub(re_emptyEg, ' ', doc)\n",
    "    doc = reg.sub(re_clickHere, ' ', doc)\n",
    "    doc = reg.sub(re_email, ' ', doc)\n",
    "    doc = reg.sub(re_emptyParens, ' ', doc)\n",
    "    doc = reg.sub(re_emptySee, ' ', doc)\n",
    "    doc = reg.sub(re_arxivHeader, ' ', doc)\n",
    "    doc = reg.sub(re_vixraHeader, ' ', doc)\n",
    "    \n",
    "    #This work supported by --> all the way to end of document\n",
    "    #Only remove this when it appears in the second half of the article\n",
    "    removeSupported = False\n",
    "    for m in reg.finditer(re_sponsors, doc):\n",
    "        if m.start()>(len(doc)/2):\n",
    "            #print('************',m.start(), len(doc))\n",
    "            doc = reg.sub(re_sponsors, ' ', doc)\n",
    "    \n",
    "    #Handling hyphens - 2-28-2018\n",
    "    for m in reg.finditer(re_hyphenatedWords, doc):\n",
    "        match=m.group(0)\n",
    "        \n",
    "        mergedWord = match.replace(' ', '').replace('-','')\n",
    "        if mergedWord in nlp.vocab: \n",
    "            \n",
    "            doc = doc.replace(match, mergedWord)\n",
    "        else:\n",
    "            allWords = True\n",
    "            for i in match.replace(' ', '').split('-'):\n",
    "                allWords = allWords and (i in nlp.vocab)\n",
    "            if allWords:\n",
    "                doc = doc.replace(match,(match.replace(' ', '')) )\n",
    "            else:\n",
    "                doc = doc.replace(match, mergedWord)\n",
    "    \n",
    "    return doc\n",
    "  \n",
    "    \n",
    "def find_matches(allDocs,regPat):\n",
    "    '''\n",
    "    Use this for debugging to find matches\n",
    "    '''\n",
    "    c=0\n",
    "    for i, doc in enumerate(allDocs):\n",
    "        for m in reg.finditer(regPat, doc):\n",
    "            print(i, m.start(), len(doc))\n",
    "            c=c+1\n",
    "    print('length:', c)\n",
    "\n",
    "\n",
    "def get_symbol_and_type(s):\n",
    "    '''\n",
    "    For LateX Symbols/Integers/Rational, return value of symbol and symbol type\n",
    "    '''\n",
    "    symbol = s[s.find(\"(\")+1:s.find(\")\")]\n",
    "    symbolType = s[0:3]\n",
    "    return symbol, symbolType\n",
    "\n",
    "\n",
    "def latexParsing(token, tokenPos):\n",
    "    '''\n",
    "    LateX parsing function for DIM files\n",
    "    '''\n",
    "    lastPos = 0\n",
    "    # Each line gets stored as a separate li in the list\n",
    "    l_depTokens = []\n",
    "    l_posTokens = []\n",
    "    l_morTokens = []\n",
    "    stringRep = ''\n",
    "    \n",
    "    # Try parsing the latex code as is\n",
    "    try:\n",
    "        expr = parse_latex(latexMap[token.text])\n",
    "        stringRep = srepr(expr)\n",
    "    except:\n",
    "        # Good chance the problem is the leading and trailing parens - remove them and try again\n",
    "        try: \n",
    "            expr = parse_latex(latexMap[token.text].lstrip('(').rstrip(')'))\n",
    "            stringRep = srepr(expr)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    ## If we have a sympy string representation...\n",
    "    if stringRep !='':\n",
    "        \n",
    "        # Problematic artefact from sympy parsing\n",
    "        \n",
    "        stringRep = stringRep.replace(\", precision=53\",\"\")\n",
    "        \n",
    "        # Call gov_dep function to get list of dependencies, objects\n",
    "        l_dependencies = (gov_dep(stringRep))\n",
    "        \n",
    "        # This will store each dependency item\n",
    "        dictAll = {}\n",
    "        \n",
    "        \n",
    "        ## If we actually have a list of items rather than a single symbol/integer\n",
    "        if len(l_dependencies)>0:\n",
    "        \n",
    "            #Do the D in DIM\n",
    "            for li in l_dependencies:\n",
    "\n",
    "                head=li[0]\n",
    "                tail=li[1]\n",
    "                #print('head:',head, '---', 'tail:',tail)\n",
    "\n",
    "                dictAll[head[1]]=head[0]\n",
    "                dictAll[tail[1]]=tail[0]\n",
    "                \n",
    "                #if 'Float(' in tail[0]:\n",
    "                #    print(tail[0])\n",
    "                \n",
    "                l_depTokens.append( ( get_rel(head[0]) , (head[0], head[1]+tokenPos-1), (tail[0], tail[1]+tokenPos-1) ) )\n",
    "                \n",
    "                #Keep track of govs for debugging \n",
    "                listGovs.append(head[0])\n",
    "                listGovs.append(tail[0])\n",
    "                \n",
    "                if head[1]> lastPos:\n",
    "                    lastPos = head[1]\n",
    "                \n",
    "                if tail[1]> lastPos:\n",
    "                    lastPos = tail[1]\n",
    "\n",
    "            \n",
    "        ## We're dealing with just a symbol or integer\n",
    "        else: \n",
    "            \n",
    "            #Keep track of govs for debugging \n",
    "            listGovs.append(stringRep)\n",
    "            dictAll[1]=stringRep\n",
    "            lastPos = 1\n",
    "            \n",
    "        ## Do the IM in DIM\n",
    "        for key, val in dictAll.items():\n",
    "            \n",
    "            # IF it's a symbol/integer\n",
    "            if '(' in val:\n",
    "                symbol, symbolType =get_symbol_and_type(val)\n",
    "                #if symbolType =='Flo':\n",
    "                #    symbol = str(round(float(symbol.replace(\"'\", \"\")), 4))\n",
    "                l_posTokens.append('{}\\t{}_{}'.format(key, symbol,symbolType[0:5].upper()))\n",
    "                l_morTokens.append(symbol)\n",
    "            else:\n",
    "                thisPos = 'LATEX'\n",
    "                if val in ['Equality','StrictGreaterThan','StrictLessThan','Approx','approx']:\n",
    "                    thisPos = \"COMPARE\"\n",
    "                elif val in ['Mul','Add','Pow']:\n",
    "                    thisPos = \"COMBINE\"\n",
    "                elif val in ['Function']:\n",
    "                    thisPos = \"FUNCTION\"\n",
    "                else:\n",
    "                    thisPos = \"TRANSFORM\"\n",
    "                \n",
    "                l_posTokens.append('{}\\t{}_{}'.format(key, val, thisPos))\n",
    "                l_morTokens.append(val)\n",
    "\n",
    "    return l_depTokens, l_posTokens, l_morTokens\n",
    "\n",
    "\n",
    "def find_parens(s):\n",
    "    '''\n",
    "    '''\n",
    "    toret = OrderedDict()\n",
    "    pstack = []\n",
    "\n",
    "    for i, c in enumerate(s):\n",
    "        if c == '(':\n",
    "            pstack.append(i)\n",
    "        elif c == ')':\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: {} for string: {}\".format(i, s))\n",
    "            toret[pstack.pop()] = i\n",
    "\n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: {} for string: {}\".format(i, s))\n",
    "\n",
    "    return OrderedDict(sorted(toret.items()))\n",
    "\n",
    "def gov_dep(s, i=1):\n",
    "    '''\n",
    "    '''\n",
    "    results = []\n",
    "    \n",
    "    # ignore inputs that don't match the formula syntax - there are some \"true\" values here, and we don't want to \n",
    "    # recurse into our \"Symbol('x')\" etc. tokens\n",
    "    if \"(\" in s and not s.startswith(\"'\"):\n",
    "        # Get an OrderedDict of all our parentheses pairs\n",
    "        parens = find_parens(s)\n",
    "\n",
    "        # get our parent/governor token\n",
    "        p1 = next(iter(parens))\n",
    "        p2 = parens.pop(p1)\n",
    "        \n",
    "        # if it's \"Function\" we need to include the next parenthetical(s) in the title\n",
    "        # and skip it/them so we don't try to interpret the contents as dependencies\n",
    "        if s[:p1] == \"Function\":\n",
    "            gov = (s[:p2+1], i)\n",
    "            \n",
    "            while True:\n",
    "                p1 = next(iter(parens))\n",
    "                p_2 = parens.pop(p1)\n",
    "                \n",
    "                if p1 > p2:\n",
    "                    p2 = p_2\n",
    "                    break\n",
    "        else:\n",
    "            \n",
    "            gov = (s[:p1], i)\n",
    "            \n",
    "        # Once we've got our parent/governor, grab the children/dependents\n",
    "        # and add those dependencies to our list\n",
    "        while parens:\n",
    "            # get next token as a child/dependent\n",
    "            p3 = next(iter(parens))\n",
    "            p4 = parens.pop(p3)\n",
    "            \n",
    "            # if there's a ', ' preceding us we need to index from that, not the \n",
    "            # parent parenthesis mark\n",
    "            if \", \" in s[:p3]:\n",
    "                dep_p1 = s[:p3].rfind(\", \")+2\n",
    "            else:\n",
    "                dep_p1 = p1+1\n",
    "\n",
    "            # Again, if this is 'Function' include the next parenthetical portion\n",
    "            if s[dep_p1:p3] == \"Function\":\n",
    "                dep = (s[dep_p1:p4+1], i+1)\n",
    "                \n",
    "                while True:\n",
    "                    p3 = next(iter(parens))\n",
    "                    p_4 = parens.pop(p3)\n",
    "\n",
    "                    if p3 > p4:\n",
    "                        p4 = p_4\n",
    "                        break\n",
    "            else:                  \n",
    "                ##NQ - getting value of certain things\n",
    "                if s[dep_p1:p3] in ['Symbol', 'Integer', 'Float']:\n",
    "                    dep = (s[dep_p1:p4+1], i+1)\n",
    "                else:\n",
    "                    dep = (s[dep_p1:p3], i+1)\n",
    "                    \n",
    "\n",
    "            # add dependency pair to results!\n",
    "            results.append((gov, dep))\n",
    "\n",
    "            # if there are more tokens\n",
    "            if len(parens) > 0:\n",
    "                # and the next is a child/dependent of the current child/dependent token\n",
    "                if next(iter(parens)) < p4:\n",
    "                    # recurse\n",
    "                    results += gov_dep(s[dep_p1:p4+1], i+1)\n",
    "                    \n",
    "                    # and then clean up the parentheticals we just covered recursively \n",
    "                    # so we don't try to parse them again at this level\n",
    "                    for p in [key for key in parens if key < p4]:\n",
    "                        del parens[p]\n",
    "            \n",
    "            # keep track of our token counts so we're numbering things right\n",
    "            i += len(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_rel(gov):\n",
    "    # Not yet exhaustive of options - but we need to add the relation to the dependencies for a \n",
    "    # final format of: relation(gov-#, dep-#)\n",
    "    if gov in ['Equality','StrictGreaterThan','StrictLessThan','Approx','approx']:\n",
    "        rel = \"compare\"\n",
    "    elif gov in ['Mul','Add','Pow']:\n",
    "        rel = \"combine\"\n",
    "    elif gov in ['Function']:\n",
    "        rel = \"function\"\n",
    "    else:\n",
    "        rel = \"transform\"\n",
    "    \n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# thisDocumentData = create_parse_files(nlp(allDocs2[1540]), 9999, True, '..\\\\..\\\\data\\\\processed with equations2-28\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonObj, allDocs = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    allDocsClean = pickle.load(open('allDocsClean.pkl', \"rb\" ))\n",
    "    print('Loaded pickle!')\n",
    "except:\n",
    "    print('Starting from scratch')\n",
    "    allDocsClean= []\n",
    "    for i, doc in enumerate(allDocs):\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "        if i%100==0:\n",
    "            gc.collect()\n",
    "            print(i)\n",
    "        allDocsClean.append(clean_doc(doc))\n",
    "        \n",
    "    with open('allDocsClean.pkl', 'wb') as f:\n",
    "        pickle.dump(allDocsClean, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract LateX, store it in latexMap (dictionary), and replace it with LateXEquation##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allDocs2 = [extract_and_replace_latex(doc, docNum) for docNum, doc in enumerate(allDocsClean)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy NLP pipeline setup, and NLPify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms = tuple(latexMap.keys())\n",
    "# try:\n",
    "#     entity_matcher = EntityMatcher(nlp, terms, 'EQUATION')\n",
    "#     nlp.add_pipe(entity_matcher, before='ner')\n",
    "#     #print(nlp.pipe_names) \n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "\n",
    "#### Add LateXEquations to NlP lexeme\n",
    "for key in latexMap.keys():\n",
    "    lex = nlp.vocab[key]\n",
    "\n",
    "#### If I want to remove the pipeline for any reason...\n",
    "#nlp.remove_pipe('entity_matcher')\n",
    "\n",
    "\n",
    "#### NLPify documents\n",
    "# allDocs3= []\n",
    "# for i, doc in enumerate(allDocs2[1500:1600]):\n",
    "#     print(i)\n",
    "#     allDocs3.append(nlp(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DIM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentData = create_parse_files(allDocs3, False, '..\\\\..\\\\data\\\\processed with equations2-27_3\\\\')\n",
    "\n",
    "import gc\n",
    "documentsData = []\n",
    "for i, doc in enumerate(allDocs2):\n",
    "    if i%100==0:\n",
    "        gc.collect()\n",
    "    # Send things in batches\n",
    "    nlpifiedDoc = nlp(doc)\n",
    "    thisDocumentData = create_parse_files(nlpifiedDoc, i, True, r'W:\\DARPA_ASKE\\CONSULTING\\Nasser\\Processed2-28')\n",
    "    documentsData.append(thisDocumentData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Keep this to fix hyphenated words -- DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_matches2(doc, regpat):\n",
    "#     for m in reg.finditer(regpat, doc):\n",
    "#         print( m.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regPat = '\\S(?=\\S*[-]\\s)([a-zA-Z-]+)(\\s)[A-za-z]+'\n",
    "# find_matches2(allDocs[0], regPat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempDoc = allDocs[0]\n",
    "\n",
    "# for m in reg.finditer(regPat, tempDoc):\n",
    "#     match=m.group(0)\n",
    "    \n",
    "    \n",
    "#     # Check to see if these are all words individually \n",
    "#     allWords = True\n",
    "#     for i in match.replace(' ', '').split('-'):\n",
    "#         allWords = allWords and (i in nlp.vocab)\n",
    "    \n",
    "#     if allWords: ##all words are individually words - now see if they combine to make a word\n",
    "#         mergedWord = match.replace(' ', '').replace('-','')\n",
    "#         print('merged word:', mergedWord)\n",
    "#         if mergedWord in nlp.vocab: \n",
    "#             doc.replace(match, mergedWord)\n",
    "#         else:\n",
    "            \n",
    "#             if mergedWord in nlp.vocab:\n",
    "#                 doc.replace(match, mergedWord)\n",
    "#     else:\n",
    "#         print('replacing with:', ''.join(match.replace(' ', '').split('-')))\n",
    "#         doc.replace(match, ''.join(match.replace(' ', '').split('-')))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "#     #print(match, allWords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempDoc = allDocs[0]\n",
    "\n",
    "# for m in reg.finditer(regPat, tempDoc):\n",
    "#     match=m.group(0)\n",
    "    \n",
    "#     mergedWord = match.replace(' ', '').replace('-','')\n",
    "#     if mergedWord in nlp.vocab: \n",
    "#         doc.replace(match, mergedWord)\n",
    "#     else:\n",
    "#         allWords = True\n",
    "#         for i in match.replace(' ', '').split('-'):\n",
    "#             allWords = allWords and (i in nlp.vocab)\n",
    "#         if allWords:\n",
    "#             doc.replace(match,(match.replace(' ', '')) )\n",
    "#         else:\n",
    "#             print(mergedWord)\n",
    "#             doc.replace(match, mergedWord)\n",
    "            \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multivac",
   "language": "python",
   "name": "multivac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
